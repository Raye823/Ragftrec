# ======================================
# 基本实验设置
# ======================================
model: FTragrec
dataset: Amazon_Beauty

# ======================================
# 数据集配置
# ======================================
field_separator: "\t"
seq_separator: " "
USER_ID_FIELD: user_id
ITEM_ID_FIELD: item_id
RATING_FIELD: rating
TIME_FIELD: timestamp
NEG_PREFIX: neg_
ITEM_LIST_LENGTH_FIELD: item_length
LIST_SUFFIX: _list
MAX_ITEM_LIST_LENGTH: 50
POSITION_FIELD: position_id

# ======================================
# 数据加载与过滤设置
# ======================================
# 加载列设置
load_col:
    inter: [user_id, item_id, rating, timestamp]

# 数据过滤设置
min_user_inter_num: 5    # 用户最少交互次数
min_item_inter_num: 5    # 物品最少交互次数

# ======================================
# 训练参数
# ======================================
epochs: 10              # 训练轮数
train_batch_size: 1024   # 训练批次大小
learner: adam            # 优化器
learning_rate: 0.001     # 学习率
training_neg_sample_num: 0  # CE损失不需要负采样
eval_step: 1             # 每训练1轮评估一次
stopping_step: 10        # 10轮无提升则停止
weight_decay: 0.0003     # L2正则化强度

# ======================================
# 评估参数
# ======================================
eval_batch_size: 256     # 评估批次大小
topk: [5, 10, 20, 50]    # 评估的K值列表
metrics: ['Recall', 'NDCG']  # 评估指标
valid_metric: NDCG@10     # 用于早停的验证指标
eval_setting: TO_LS,full # 按时间排序，留一法划分，全排序

# ======================================
# Transformer模型参数 
# ======================================
n_layers: 2              # Transformer层数
n_heads: 2               # 注意力头数
hidden_size: 64          # 隐藏层维度
inner_size: 256          # 内部前馈网络大小
hidden_dropout_prob: 0.5 # 隐藏层dropout
attn_dropout_prob: 0.5   # 注意力dropout
hidden_act: 'gelu'       # 激活函数
layer_norm_eps: 1e-12    # 层归一化参数
initializer_range: 0.02  # 初始化范围
loss_type: 'CE'          # 使用交叉熵损失

# ======================================
# 对比学习参数（与DuoRec兼容）
# ======================================
contrast: 'us_x'         # 对比学习类型
tau: 1                   # 温度参数
sim: 'dot'               # 相似度计算方式
lmd: 0.1                 # lambda参数
lmd_sem: 0.1             # 语义lambda参数

# ======================================
# 检索参数设置
# ======================================
# 基础检索参数
len_lower_bound: -1      # 序列长度下限（-1表示不限制）
len_upper_bound: -1      # 序列长度上限（-1表示不限制）
len_bound_reverse: False # 反向边界（False表示保留符合条件的序列）
nprobe: 1               # FAISS探测数量
top_k: 10                 # 检索的序列数量

# 检索器编码器参数
retriever_layers: 1      # 检索器层数
retriever_temperature: 0.07  # 检索温度参数
retriever_dropout: 0.5   # 检索器dropout
retriever_update_interval: 11 # 检索器更新间隔
kl_weight: 0          # KL散度损失权重 (设为大于0的值以启用JS散度优化)

# ======================================
# 路径和预训练设置
# ======================================
log_root: "./log/"       # 日志根目录
data_path: "./recbole/dataset/"  # 数据集路径
pretrain: False          # 是否使用预训练

# ======================================
# 检索增强核心参数
# ======================================
# 混合策略参数
alpha: 0.15              # 检索知识混合比例（15%检索知识）
enhanced_rec_weight: 0 # 增强推荐损失权重

# 预测阶段检索增强
use_retrieval_for_predict: True  # 预测时是否使用检索增强
predict_retrieval_alpha: 0.15    # 预测时的检索增强混合比例
predict_retrieval_temperature: 0.07  # 预测时的注意力温度参数

# ======================================
# 可微分记忆模块参数
# ======================================
use_diff_memory: True     # 是否启用可微分记忆模块
memory_size: 2048         # 记忆库大小
memory_weight: 1        # 记忆模块损失权重
diff_temperature: 0.05    # 记忆检索温度参数
