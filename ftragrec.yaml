# 实验设置
model: FTragrec
dataset: Amazon_Beauty

# 数据集配置
field_separator: "\t"
seq_separator: " "
USER_ID_FIELD: user_id
ITEM_ID_FIELD: item_id
RATING_FIELD: rating
TIME_FIELD: timestamp
NEG_PREFIX: neg_
ITEM_LIST_LENGTH_FIELD: item_length
LIST_SUFFIX: _list
MAX_ITEM_LIST_LENGTH: 50
POSITION_FIELD: position_id

# 加载列设置
load_col:
    inter: [user_id, item_id, rating, timestamp]

# 数据过滤设置
min_user_inter_num: 5
min_item_inter_num: 5

# 训练参数
epochs: 100
train_batch_size: 1024
learner: adam
learning_rate: 0.001  # 适中学习率
training_neg_sample_num: 0  # CE损失不需要负采样
eval_step: 1
stopping_step: 10  # 适当延长停止步数
weight_decay: 0.0003  # 适中正则化强度

# 评估参数
eval_batch_size: 256
topk: [5, 10, 20, 50]
metrics: ['Recall', 'MRR', 'NDCG']
valid_metric: MRR@10
eval_setting: TO_LS,full  # 按时间排序，留一法划分，全排序

# 模型参数 - 与DuoRec保持一致
n_layers: 2
n_heads: 2
hidden_size: 64
inner_size: 256
hidden_dropout_prob: 0.5
attn_dropout_prob: 0.5
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02
loss_type: 'CE'  # 使用CE损失

# 与DuoRec兼容的参数
contrast: 'us_x'  # 与DuoRec保持一致
tau: 1
sim: 'dot'
lmd: 0.1
lmd_sem: 0.1

# 检索参数
len_lower_bound: -1  # 只使用长度≥5的序列作为检索源
len_upper_bound: -1
len_bound_reverse: False  # 保留符合条件的序列
nprobe: 10  # 合理的探测数量
top_k: 5  # 适中的检索数量

# RetrieverEncoder参数
retriever_layers: 2  # 保留一层检索器
retriever_temperature: 0.07  # 适中温度参数
retriever_dropout: 0.2  # 适中dropout，平衡泛化与保留知识
retriever_update_interval: 6  # 减少更新频率，保持检索稳定性
kl_weight: 0.2  # 降低KL散度损失权重但不完全忽略

# 目录设置
log_root: "./log/"
data_path: "./recbole/dataset/"

# 预训练设置
pretrain: False  # 由命令行参数控制

# 微调检索增强参数 - 关键部分
alpha: 0.15  # 轻微检索知识混合，保持原始表示为主
enhanced_rec_weight: 0.8  # 确保增强的推荐损失有足够权重

# 检索质量控制参数 - 新增
quality_threshold: 0.6  # 只有当检索质量高于阈值时才使用检索结果
dynamic_alpha: True  # 动态调整alpha基于检索质量
warmup_epochs: 3  # 在前几轮不使用检索增强，让模型先稳定

# 预测阶段检索增强参数
use_retrieval_for_predict: True  # 预测时使用检索增强
predict_retrieval_alpha: 0.15  # 轻微混合预测时的检索增强
predict_retrieval_temperature: 0.07  # 注意力温度参数